
<!DOCTYPE html>
<html lang="zh-Hans" class="loading">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>PCL-Learning-Notes - Dengwanxia</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate">
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="点云库(PCL:Point Cloud Library)学习笔记

1、了解名词(1)点云：通过测量仪器得到的产品外观表面的点数据集合称之为点云，使用三维激光扫描仪或照相式扫描仪得到的点云，点数量比,"> 
    <meta name="author" content="WanxiaDenng"> 
    <link rel="alternative" href="atom.xml" title="Dengwanxia" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads" src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
</head>
</html>
<body class="loading">
    <span id="config-title" style="display:none">Dengwanxia</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;" data-url="http://yoursite.com"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">PCL-Learning-Notes</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class="main">
        <h1 class="title">PCL-Learning-Notes</h1>
        <div class="stuff">
            <span>五月 18, 2019</span>
            

        </div>
        <div class="content markdown">
            <font size="3">点云库(PCL:Point Cloud Library)学习笔记</font>

<p>1、了解名词<br>(1)点云：通过测量仪器得到的产品外观表面的点数据集合称之为点云，使用三维激光扫描仪或照相式扫描仪得到的点云，点数量比较大并且比较密集，叫密集点云。</p>
<p>点云是在和目标表面特性的海量点集合。<br>根据激光测量原理得到的点云，包括三维坐标(XYZ)和激光反射强度(Intensity)。<br>根据摄影测量原理得到的点云，包括三维坐标(XYZ)和颜色信息(RGB)。<br>结合激光测量和摄影测量原理得到点云，包括三维坐标(XYZ)、激光反射强度(Intensity)和颜色信息(RGB)。<br>在获取物体表面每个采样点的空间坐标后，得到的是一个点的集合，称之为“点云”(Point Cloud)。</p>
<p>当一束激光照射到物体表面时，所反射的激光会携带方位、距离等信息。若将激光束按照某种轨迹进行扫描，便会边扫描边记录到反射的激光点信息，由于扫描极为精细，则能够得到大量的激光点，因而就可形成激光点云。</p>
<p>(2)SLAM，即时定位与制图，包含3个关键词：实时、定位、制图，就是实时完成定位和制图的任务，这就是SLAM要解决的基本任务。按照使用的传感器分为<br>激光SLAM（LOAM、V-LOAM、cartographer）与视觉SLAM，其中视觉SLAM又可分为<br>单目SLAM（MonoSLAM、PTAM、DTAM、LSD-SLAM、ORB-SLAM（单目为主）、SVO）、双目SLAM（LIBVISO2、S-PTAM等）、<br>RGBD SLAM（KinectFusion、ElasticFusion、Kintinous、RGBD SLAM2、RTAB SLAM）；<br>视觉SLAM由前端（视觉里程计）、后端（位姿优化）、闭环检测、制图4个部分组成。</p>
<p>(3)Kinect RGB-D摄像机包含三个镜头，一个彩色摄像头，一个红外发射器和一个红外线CMOS摄像头。其中彩色摄像头可以获取彩色图像，红外线发射器和红外线CMOS摄像头共同组成了三维深度传感器。使用Kinect可以得到一副RGB图像和与之对应的深度图像，两种图像可以合成点云图。</p>
<p>2、kinect标定<br>视觉SLAM中通过相机模型来生成三维点云数据，需要对Kinect进行标定。Kinetc能同时获取彩色图像和深度图像，需要分别对两种图像进行标定。<br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-1.png" alt="waitting"><br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-2.png" alt="waitting"><br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-3.png" alt="waitting"></p>
<p>3、点线特征提取</p>
<p>(1)基于点线特征的三维地图构建算法主要包含三个模块：前端视觉里程计、后端优化和闭环优化。视觉里程计实时处理视频流，通过点线特征匹配求解相机的当前位姿；后端优化维护局部地图并对视觉里程计估计的初始位姿进行局部优化；闭环优化在检测到闭环后，通过闭环校正和全局优化得到全局一致的轨迹和地图。(视觉里程计：视觉里程计（VO），它根据相邻图像的信息粗略的估计出相机的运动，给后端提供较好的初始值。VO的实现方法可以根据是否需要提取特征分为两类：基于特征点的方法，不使用特征点的直接方法。 基于特征点的VO运行稳定，对光照、动态物体不敏感。)</p>
<p>(2)点特征提取<br>1)FAST特征点提取算法<br>FAST算法的思想很简单：如果一个像素与周围邻域的像素差别较大（过亮或者过暗），那么可以认为该像素是一个角点。和其他的特征点提取算法相比，FAST算法只需要比较像素和其邻域像素的灰度值大小，十分便捷。<br>FAST算法提取角点的步骤：<br><strong><font size="3">·</font></strong>在图像中选择像素p，假设其灰度值为：Ip<br><strong><font size="3">·</font></strong>设置一个阈值T，例如：Ip的20%<br><strong><font size="3">·</font></strong>选择p周围半径为3的圆上的16个像素，作为比较像素<br><strong><font size="3">·</font></strong>假设选取的圆上有连续的N个像素大于Ip+T或者Ip−T，那么可以认为像素p就是一个特征点。（N通常取12，即为FAST-12；常用的还有FAST-9,FAST-11）。FAST算法只检测像素的灰度值，其运算速度极快，同时不可避免的也有一些缺点。<br><strong><font size="3">·</font></strong>检测到的特征点过多并且会出现“扎堆”的现象。这可以在第一遍检测完成后，使用非最大值抑制（Non-maximal suppression），在一定区域内仅保留响应极大值的角点，避免角点集中的情况。<br><strong><font size="3">·</font></strong>FAST提取到的角点没有方向和尺度信息。</p>
<p>2)BRIEF描述子<br>BRIEF是一种二进制的描述子，其描述向量是0和1表示的二进制串。0和1表示特征点邻域内两个像素（p和q）灰度值的大小：如果p比q大则选择1，反正就取0。在特征点的周围选择128对这样的p和q的像素对，就得到了128维由0，1组成的向量。那么p和q的像素对是怎么选择的呢？通常都是按照某种概率来随机的挑选像素对的位置。<br>BRIEF使用随机选点的比较，速度很快，而且使用二进制串表示最终生成的描述子向量，在存储以及用于匹配的比较时都是非常方便的，其和FAST的搭配起来可以组成非常快速的特征点提取和描述算法。</p>
<p>3)ORB算法<br>ORB是目前来说非常好的能够进行的实时的图像特征提取和描述的算法，它改进了FAST特征提取算法，并使用速度极快的二进制描述子BRIEF。针对FAST特征提取的算法的一些缺点，ORB也做了相应的改进。<br><strong><font size="3">·</font></strong>使用非最大值抑制，在一定区域内仅仅保留响应极大值的角点，避免FAST提取到的角点过于集中。<br><strong><font size="3">·</font></strong>FAST提取到的角点数量过多且不是很稳定，ORB中可以指定需要提取到的角点的数量N，然后对FAST提取到的角点分别计算Harris响应值，选择前N个具有最大响应值的角点作为最终提取到的特征点集合。<br><strong><font size="3">·</font></strong>FAST提取到的角点不具有尺度信息，在ORB中使用图像金字塔，并且在每一层金字塔上检测角点，以此来保持尺度的不变性。<br><strong><font size="3">·</font></strong>FAST提取到的角点不具有方向信息，在ORB中使用灰度质心法(Intensity Centroid)来保持特征的旋转不变性。</p>
<p>4)OpenCV3中特征点的提取和匹配<br>OpenCV中封装了常用的特征点算法（如SIFT,SURF，ORB等），提供了统一的接口，便于调用。 下面代码是OpenCV中使用其feature 2D 模块的示例代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Mat img1 = imread(&quot;F:\\image\\1.png&quot;);</span><br><span class="line">Mat img2 = imread(&quot;F:\\image\\2.png&quot;);</span><br><span class="line"></span><br><span class="line">// 1. 初始化</span><br><span class="line">vector&lt;KeyPoint&gt; keypoints1, keypoints2;</span><br><span class="line">Mat descriptors1, descriptors2;</span><br><span class="line">Ptr&lt;ORB&gt; orb = ORB::create();</span><br><span class="line"></span><br><span class="line">// 2. 提取特征点</span><br><span class="line">orb-&gt;detect(img1, keypoints1);</span><br><span class="line">orb-&gt;detect(img2, keypoints2);</span><br><span class="line"></span><br><span class="line">// 3. 计算特征描述符</span><br><span class="line">orb-&gt;compute(img1, keypoints1, descriptors1);</span><br><span class="line">orb-&gt;compute(img2, keypoints2, descriptors2);</span><br><span class="line">    </span><br><span class="line">// 4. 对两幅图像的BRIEF描述符进行匹配，使用BFMatch，Hamming距离作为参考</span><br><span class="line">vector&lt;DMatch&gt; matches;</span><br><span class="line">BFMatcher bfMatcher(NORM_HAMMING);</span><br><span class="line">bfMatcher.match(descriptors1, descriptors2, matches);</span><br></pre></td></tr></table></figure></p>
<p><strong><font size="3">·</font></strong>在OpenCV3中重新的封装了特征提取的接口，可统一的使用Ptr<featuredetector> detector = FeatureDetector::create()来得到特征提取器的一个实例，所有的参数都提供了默认值，也可以根据具体的需要传入相应的参数。<br><strong><font size="3">·</font></strong>在得到特征检测器的实例后，可调用的detect方法检测图像中的特征点的具体位置，检测的结果保存在vector<keypoint>向量中。<br><strong><font size="3">·</font></strong>有了特征点的位置后，调用compute方法来计算特征点的描述子，描述子通常是一个向量，保存在Mat中。<br><strong><font size="3">·</font></strong>得到了描述子后，可调用匹配算法进行特征点的匹配。上面代码中，使用了opencv中封装后的暴力匹配算法BFMatcher，该算法在向量空间中，将特征点的描述子一一比较，选择距离（上面代码中使用的是Hamming距离）较小的一对作为匹配点。<br>上面代码匹配后的结果如下：<br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-4.jpg" alt="waitting"><br>特征点的匹配后的优化:<br>特征的匹配是针对特征描述子的进行的，上面提到特征描述子通常是一个向量，两个特征描述子的之间的距离可以反应出其相似的程度，也就是这两个特征点是不是同一个。根据描述子的不同，可以选择不同的距离度量。如果是浮点类型的描述子，可以使用其欧式距离；对于二进制的描述子（BRIEF）可以使用其汉明距离（两个不同二进制之间的汉明距离指的是两个二进制串不同位的个数）。</keypoint></featuredetector></p>
<p>有了计算描述子相似度的方法，那么在特征点的集合中如何寻找和其最相似的特征点，这就是特征点的匹配了。最简单直观的方法就是上面使用的：暴力匹配方法(Brute-Froce Matcher)，计算某一个特征点描述子与其他所有特征点描述子之间的距离，然后将得到的距离进行排序，取距离最近的一个作为匹配点。这种方法简单粗暴，其结果也是显而易见的，通过上面的匹配结果，也可以看出有大量的错误匹配，这就需要使用一些机制来过滤掉错误的匹配。</p>
<p><strong><font size="3">·</font></strong>汉明距离小于最小距离的两倍:选择已经匹配的点对的汉明距离小于最小距离的两倍作为判断依据，如果小于该值则认为是一个错误的匹配，过滤掉；大于该值则认为是一个正确的匹配。其实现代码也很简单，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 匹配对筛选</span><br><span class="line">double min_dist = 1000, max_dist = 0;</span><br><span class="line">// 找出所有匹配之间的最大值和最小值</span><br><span class="line">for (int i = 0; i &lt; descriptors1.rows; i++)</span><br><span class="line">&#123;</span><br><span class="line">	double dist = matches[i].distance;</span><br><span class="line">    if (dist &lt; min_dist) min_dist = dist;</span><br><span class="line">    if (dist &gt; max_dist) max_dist = dist;</span><br><span class="line">&#125;</span><br><span class="line">// 当描述子之间的匹配大于2倍的最小距离时，即认为该匹配是一个错误的匹配。</span><br><span class="line">// 但有时描述子之间的最小距离非常小，可以设置一个经验值作为下限</span><br><span class="line">vector&lt;DMatch&gt; good_matches;</span><br><span class="line">for (int i = 0; i &lt; descriptors1.rows; i++)</span><br><span class="line">&#123;</span><br><span class="line">    if (matches[i].distance &lt;= max(2 * min_dist, 30.0))</span><br><span class="line">    good_matches.push_back(matches[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>结果如下：<br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-5.jpg" alt="waitting"><br>对比只是用暴力匹配的方法，进行过滤后的匹配效果好了很多。</p>
<p><strong><font size="3">·</font></strong>交叉匹配<br>针对暴力匹配，可以使用交叉匹配的方法来过滤错误的匹配。交叉过滤的是想很简单，再进行一次匹配，反过来使用被匹配到的点进行匹配，如果匹配到的仍然是第一次匹配的点的话，就认为这是一个正确的匹配。举例来说就是，假如第一次特征点A使用暴力匹配的方法，匹配到的特征点是特征点B；反过来，使用特征点B进行匹配，如果匹配到的仍然是特征点A，则就认为这是一个正确的匹配，否则就是一个错误的匹配。OpenCV中BFMatcher已经封装了该方法，创建BFMatcher的实例时，第二个参数传入true即可，BFMatcher bfMatcher(NORM_HAMMING,true)。</p>
<p><strong><font size="3">·</font></strong>KNN匹配<br>K近邻匹配，在匹配的时候选择K个和特征点最相似的点，如果这K个点之间的区别足够大，则选择最相似的那个点作为匹配点，通常选择K = 2，也就是最近邻匹配。对每个匹配返回两个最近邻的匹配，如果第一匹配和第二匹配距离比率足够大（向量距离足够远），则认为这是一个正确的匹配，比率的阈值通常在2左右。OpenCV中的匹配器中封装了该方法，上面的代码可以调用bfMatcher-&gt;knnMatch(descriptors1, descriptors2, knnMatches, 2);具体实现的代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">const float minRatio = 1.f / 1.5f;</span><br><span class="line">const int k = 2;</span><br><span class="line"></span><br><span class="line">vector&lt;vector&lt;DMatch&gt;&gt; knnMatches;</span><br><span class="line">matcher-&gt;knnMatch(leftPattern-&gt;descriptors, rightPattern-&gt;descriptors, knnMatches, k);</span><br><span class="line"></span><br><span class="line">for (size_t i = 0; i &lt; knnMatches.size(); i++) &#123;</span><br><span class="line">	const DMatch&amp; bestMatch = knnMatches[i][0];</span><br><span class="line">	const DMatch&amp; betterMatch = knnMatches[i][1];</span><br><span class="line"></span><br><span class="line">	float  distanceRatio = bestMatch.distance / betterMatch.distance;</span><br><span class="line">	if (distanceRatio &lt; minRatio)</span><br><span class="line">            matches.push_back(bestMatch);</span><br><span class="line">&#125;const  float minRatio =  1.f  /  1.5f;</span><br><span class="line">const  int k =  2;</span><br><span class="line"></span><br><span class="line">vector&lt;vector&lt;DMatch&gt;&gt; knnMatches;</span><br><span class="line">matcher-&gt;knnMatch(leftPattern-&gt;descriptors, rightPattern-&gt;descriptors, knnMatches, 2);</span><br><span class="line"></span><br><span class="line">for (size_t i =  0; i &lt; knnMatches.size(); i++) &#123;</span><br><span class="line">    const DMatch&amp; bestMatch = knnMatches[i][0];</span><br><span class="line">    const DMatch&amp; betterMatch = knnMatches[i][1];</span><br><span class="line">    float distanceRatio = bestMatch.distance  / betterMatch.distance;</span><br><span class="line">    if (distanceRatio &lt; minRatio)</span><br><span class="line">            matches.push_back(bestMatch);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>将不满足的最近邻的匹配之间距离比率大于设定的阈值（1/1.5）匹配剔除。</p>
<p><strong><font size="3">·</font></strong>RANSAC:另外还可采用随机采样一致性（RANSAC）来过滤掉错误的匹配，该方法利用匹配点计算两个图像之间单应矩阵，然后利用重投影误差来判定某一个匹配是不是正确的匹配。OpenCV中封装了求解单应矩阵的方法findHomography,可以为该方法设定一个重投影误差的阈值，可以得到一个向量mask来指定那些是符合该重投影误差的匹配点对，以此来剔除错误的匹配，代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">const int minNumbermatchesAllowed = 8;</span><br><span class="line">if (matches.size() &lt; minNumbermatchesAllowed)</span><br><span class="line">    return;</span><br><span class="line"></span><br><span class="line">//Prepare data for findHomography</span><br><span class="line">vector&lt;Point2f&gt; srcPoints(matches.size());</span><br><span class="line">vector&lt;Point2f&gt; dstPoints(matches.size());</span><br><span class="line"></span><br><span class="line">for (size_t i = 0; i &lt; matches.size(); i++) &#123;</span><br><span class="line">    srcPoints[i] = rightPattern-&gt;keypoints[matches[i].trainIdx].pt;</span><br><span class="line">    dstPoints[i] = leftPattern-&gt;keypoints[matches[i].queryIdx].pt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//find homography matrix and get inliers mask</span><br><span class="line">vector&lt;uchar&gt; inliersMask(srcPoints.size());</span><br><span class="line">homography = findHomography(srcPoints, dstPoints, CV_FM_RANSAC, reprojectionThreshold, inliersMask);</span><br><span class="line"></span><br><span class="line">vector&lt;DMatch&gt; inliers;</span><br><span class="line">for (size_t i = 0; i &lt; inliersMask.size(); i++)&#123;</span><br><span class="line">    if (inliersMask[i])</span><br><span class="line">        inliers.push_back(matches[i]);</span><br><span class="line">&#125;</span><br><span class="line">matches.swap(inliers);const  int minNumbermatchesAllowed =  8;</span><br><span class="line">if (matches.size() &lt; minNumbermatchesAllowed)</span><br><span class="line">    return;</span><br><span class="line"></span><br><span class="line">//Prepare data for findHomography</span><br><span class="line">vector&lt;Point2f&gt;  srcPoints(matches.size());</span><br><span class="line">vector&lt;Point2f&gt;  dstPoints(matches.size());</span><br><span class="line"></span><br><span class="line">for (size_t i =  0; i &lt; matches.size(); i++) &#123;</span><br><span class="line">    srcPoints[i] = rightPattern-&gt;keypoints[matches[i].trainIdx].pt;</span><br><span class="line">    dstPoints[i] = leftPattern-&gt;keypoints[matches[i].queryIdx].pt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//find homography matrix and get inliers mask</span><br><span class="line">vector&lt;uchar&gt;  inliersMask(srcPoints.size());</span><br><span class="line">homography =  findHomography(srcPoints, dstPoints, CV_FM_RANSAC, reprojectionThreshold, inliersMask);</span><br><span class="line"></span><br><span class="line">vector&lt;DMatch&gt; inliers;</span><br><span class="line">for (size_t i =  0; i &lt; inliersMask.size(); i++)&#123;</span><br><span class="line">    if (inliersMask[i])</span><br><span class="line">        inliers.push_back(matches[i]);</span><br><span class="line">&#125;</span><br><span class="line">matches.swap(inliers);</span><br></pre></td></tr></table></figure></p>
<p>(3)线特征提取</p>
<p>4、前端视觉里程计位姿估计</p>
<p>5、后端优化<br>视觉里程计能够增量式地构建环境地图，但其只考虑相邻之间位姿关系，长时间运行不可避免会产生累计误差，这样得到的地图是不准确的。后端优化环节把当前帧与更早的帧关联起来，加入更多的地图数据进行优化，从而降低累计误差。<br>视觉SLAM中会有成千上万帧图像，后端优化并不会耗费巨大的计算量将每一帧都纳入优化，因此需要选择一些有代表性的帧作为关键帧。同时，为了便于后续跟踪，需要尽可能快速插入关键帧。</p>
<p>6、八叉树地图的构建<br>点云地图是一种较为初级的地图表达方式，不能直接用于机器人的导航。八叉树地图中的每个节点表示一个立方体所包含的空间，通常称为体素。这个立方体递归地细分为八个子立方体，直到分割成最小的体素。最小体素的大小决定了八叉树的分辨率，由于八叉树是一种分层数据结构，能够在任意层次切割得到不同分辨率的表达。八叉树在不同分辨率下的表示如图：<br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-6.png" alt="cmd-markdown-logo"><br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-7.png" alt="cmd-markdown-logo"><br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-8.png" alt="cmd-markdown-logo"><br>由八叉树的表达方式可以看出，其优点在于：<br>(1)压缩空间：八叉树的节点存储了是否被占据的信息，当节点的所有子节点全部被占据或是全不占据时，就无需展开此节点，能够大大节省内存空间。<br>(2)更新地图：通过概率对数可以方便的对八叉树地图进行更新。<br>(3)导航与避障：八叉树地图每个体素表示其被占据的概率，机器人可以很方便地对八叉树地图进行空间可行性分析和路径规划。并且八叉树地图分辨率可调，可以灵活地将不同分辨率的地图应用于不同层次的导航。<br><img src="https://github.com/YuemingBi/comment-repo/raw/master/image/PCL-Learning-Notes/pcl-9.png" alt="cmd-markdown-logo"></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src>
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        <li title="0" data-url="http://link.hhtjim.com/163/5146554.mp3"></li>
                    
                        <li title="1" data-url="http://link.hhtjim.com/qq/001faIUs4M2zna.mp3"></li>
                    
                </ul>
            
        </div>
        
    <div id="gitalk-container" class="comment link" data-ae="false" data-ci data-cs data-r data-o data-a data-d="false">查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>
